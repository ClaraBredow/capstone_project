{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "import datetime, time, os\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "    \n",
    "print('Using TensorFlow version: %s' % tf.__version__)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import datetime, time, os\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from src.preprocessing.preprocessing_functions import drop_rows\n",
    "\n",
    "#!pip install -q git+https://github.com/tensorflow/docs # install first time\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fafe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this command you can clear any logs from previous runs\n",
    "# If you want to compare different runs you can skip this cell \n",
    "!rm -rf my_logs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a12ef62",
   "metadata": {},
   "source": [
    "## Loading the processed dataframe (sugarbeet and openweather station data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb56623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pickles/df_fieldweather_devstage_sugarbeet.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to string for categorizing\n",
    "df['seednames_coded'] = df['seednames_coded'].astype(str)\n",
    "df['pollinator_comp'] = df['pollinator_comp'].astype(str)\n",
    "df['ms_comp'] = df['ms_comp'].astype(str)\n",
    "df['otype_comp'] = df['otype_comp'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d2078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce72e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.station_location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop locations for comparison\n",
    "drop_rows(df, 'station_location', ['Bautzen', 'Goderville', 'Lamotte', 'Lelystad', 'Pithiviers', \n",
    "                                   'Rittershausen', 'Sommepy', 'Vierhoefen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc263bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns \n",
    "df.drop(['betaine_nir', \n",
    "         'cry_nir', \n",
    "         'dm_nir', \n",
    "         'invert_nir', \n",
    "         'mark_nir', \n",
    "         #'sc_nir',\n",
    "         'csy_nir', \n",
    "         'totaln_nir',\n",
    "         'obj',  \n",
    "         'seriesid', \n",
    "         'x', \n",
    "         'y', \n",
    "         'ms_comp',\n",
    "         'otype_comp', \n",
    "         #'pollinator_comp',\n",
    "         #'seednames_coded',\n",
    "         'region',\n",
    "         'station_location'\n",
    "         ], axis=1, inplace=True)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852aa4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ec5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index after drpping columns\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f7fca90",
   "metadata": {},
   "source": [
    "#### Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list for categorical predictors/features \n",
    "cat_features = list(df.columns[df.dtypes==object])\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18542a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list for numerical predictors/features\n",
    "# since 'sc_nir' is our target variable we will exclude this feature from the list of numerical predictors \n",
    "# latitude and longitude are also excluded to avoid location influence on prediction\n",
    "num_features = [\n",
    " 'air_temperature_avg_devstage_1',\n",
    " 'air_temperature_avg_devstage_2',\n",
    " 'air_temperature_avg_devstage_3',\n",
    " 'air_temperature_max_devstage_1',\n",
    " 'air_temperature_max_devstage_2',\n",
    " 'air_temperature_max_devstage_3',\n",
    " 'air_temperature_min_devstage_1',\n",
    " 'air_temperature_min_devstage_2',\n",
    " 'air_temperature_min_devstage_3',\n",
    " 'deltat_avg_devstage_1',\n",
    " 'deltat_avg_devstage_2',\n",
    " 'deltat_avg_devstage_3',\n",
    " 'deltat_max_devstage_1',\n",
    " 'deltat_max_devstage_2',\n",
    " 'deltat_max_devstage_3',\n",
    " 'deltat_min_devstage_1',\n",
    " 'deltat_min_devstage_2',\n",
    " 'deltat_min_devstage_3',\n",
    " 'dew_point_avg_devstage_1',\n",
    " 'dew_point_avg_devstage_2',\n",
    " 'dew_point_avg_devstage_3',\n",
    " 'dew_point_min_devstage_1',\n",
    " 'dew_point_min_devstage_2',\n",
    " 'dew_point_min_devstage_3',\n",
    " 'eag_soil_moisture_1_devstage_1',\n",
    " 'eag_soil_moisture_1_devstage_2',\n",
    " 'eag_soil_moisture_1_devstage_3',\n",
    " 'eag_soil_moisture_2_devstage_1',\n",
    " 'eag_soil_moisture_2_devstage_2',\n",
    " 'eag_soil_moisture_2_devstage_3',\n",
    " 'eag_soil_moisture_3_devstage_1',\n",
    " 'eag_soil_moisture_3_devstage_2',\n",
    " 'eag_soil_moisture_3_devstage_3',\n",
    " 'eag_soil_moisture_4_devstage_1',\n",
    " 'eag_soil_moisture_4_devstage_2',\n",
    " 'eag_soil_moisture_4_devstage_3',\n",
    " 'eag_soil_moisture_5_devstage_1',\n",
    " 'eag_soil_moisture_5_devstage_2',\n",
    " 'eag_soil_moisture_5_devstage_3',\n",
    " 'eag_soil_moisture_6_devstage_1',\n",
    " 'eag_soil_moisture_6_devstage_2',\n",
    " 'eag_soil_moisture_6_devstage_3',\n",
    " 'et0_devstage_1',\n",
    " 'et0_devstage_2',\n",
    " 'et0_devstage_3',\n",
    "#  'latitude_1',\n",
    "#  'latitude_2',\n",
    "#  'latitude_3',\n",
    " 'leaf_wetness_devstage_1',\n",
    " 'leaf_wetness_devstage_2',\n",
    " 'leaf_wetness_devstage_3',\n",
    "#  'longitude_1',\n",
    "#  'longitude_2',\n",
    "#  'longitude_3',\n",
    " 'precipitation_devstage_1',\n",
    " 'precipitation_devstage_2',\n",
    " 'precipitation_devstage_3',\n",
    " 'relative_humidity_avg_devstage_1',\n",
    " 'relative_humidity_avg_devstage_2',\n",
    " 'relative_humidity_avg_devstage_3',\n",
    " 'relative_humidity_max_devstage_1',\n",
    " 'relative_humidity_max_devstage_2',\n",
    " 'relative_humidity_max_devstage_3',\n",
    " 'relative_humidity_min_devstage_1',\n",
    " 'relative_humidity_min_devstage_2',\n",
    " 'relative_humidity_min_devstage_3',\n",
    " 'saturation_vpd_avg_devstage_1',\n",
    " 'saturation_vpd_avg_devstage_2',\n",
    " 'saturation_vpd_avg_devstage_3',\n",
    " 'saturation_vpd_min_devstage_1',\n",
    " 'saturation_vpd_min_devstage_2',\n",
    " 'saturation_vpd_min_devstage_3',\n",
    " 'soil_salinity_1_devstage_1',\n",
    " 'soil_salinity_1_devstage_2',\n",
    " 'soil_salinity_1_devstage_3',\n",
    " 'soil_salinity_2_devstage_1',\n",
    " 'soil_salinity_2_devstage_2',\n",
    " 'soil_salinity_2_devstage_3',\n",
    " 'soil_salinity_3_devstage_1',\n",
    " 'soil_salinity_3_devstage_2',\n",
    " 'soil_salinity_3_devstage_3',\n",
    " 'soil_salinity_4_devstage_1',\n",
    " 'soil_salinity_4_devstage_2',\n",
    " 'soil_salinity_4_devstage_3',\n",
    " 'soil_salinity_5_devstage_1',\n",
    " 'soil_salinity_5_devstage_2',\n",
    " 'soil_salinity_5_devstage_3',\n",
    " 'soil_salinity_6_devstage_1',\n",
    " 'soil_salinity_6_devstage_2',\n",
    " 'soil_salinity_6_devstage_3',\n",
    " 'soil_temperature_1_max_devstage_1',\n",
    " 'soil_temperature_1_max_devstage_2',\n",
    " 'soil_temperature_1_max_devstage_3',\n",
    " 'soil_temperature_1_min_devstage_1',\n",
    " 'soil_temperature_1_min_devstage_2',\n",
    " 'soil_temperature_1_min_devstage_3',\n",
    " 'soil_temperature_1_vg_devstage_1',\n",
    " 'soil_temperature_1_vg_devstage_2',\n",
    " 'soil_temperature_1_vg_devstage_3',\n",
    " 'soil_temperature_2_max_devstage_1',\n",
    " 'soil_temperature_2_max_devstage_2',\n",
    " 'soil_temperature_2_max_devstage_3',\n",
    " 'soil_temperature_2_min_devstage_1',\n",
    " 'soil_temperature_2_min_devstage_2',\n",
    " 'soil_temperature_2_min_devstage_3',\n",
    " 'soil_temperature_2_vg_devstage_1',\n",
    " 'soil_temperature_2_vg_devstage_2',\n",
    " 'soil_temperature_2_vg_devstage_3',\n",
    " 'soil_temperature_3_max_devstage_1',\n",
    " 'soil_temperature_3_max_devstage_2',\n",
    " 'soil_temperature_3_max_devstage_3',\n",
    " 'soil_temperature_3_min_devstage_1',\n",
    " 'soil_temperature_3_min_devstage_2',\n",
    " 'soil_temperature_3_min_devstage_3',\n",
    " 'soil_temperature_3_vg_devstage_1',\n",
    " 'soil_temperature_3_vg_devstage_2',\n",
    " 'soil_temperature_3_vg_devstage_3',\n",
    " 'soil_temperature_4_max_devstage_1',\n",
    " 'soil_temperature_4_max_devstage_2',\n",
    " 'soil_temperature_4_max_devstage_3',\n",
    " 'soil_temperature_4_min_devstage_1',\n",
    " 'soil_temperature_4_min_devstage_2',\n",
    " 'soil_temperature_4_min_devstage_3',\n",
    " 'soil_temperature_4_vg_devstage_1',\n",
    " 'soil_temperature_4_vg_devstage_2',\n",
    " 'soil_temperature_4_vg_devstage_3',\n",
    " 'soil_temperature_5_max_devstage_1',\n",
    " 'soil_temperature_5_max_devstage_2',\n",
    " 'soil_temperature_5_max_devstage_3',\n",
    " 'soil_temperature_5_min_devstage_1',\n",
    " 'soil_temperature_5_min_devstage_2',\n",
    " 'soil_temperature_5_min_devstage_3',\n",
    " 'soil_temperature_5_vg_devstage_1',\n",
    " 'soil_temperature_5_vg_devstage_2',\n",
    " 'soil_temperature_5_vg_devstage_3',\n",
    " 'soil_temperature_6_max_devstage_1',\n",
    " 'soil_temperature_6_max_devstage_2',\n",
    " 'soil_temperature_6_max_devstage_3',\n",
    " 'soil_temperature_6_min_devstage_1',\n",
    " 'soil_temperature_6_min_devstage_2',\n",
    " 'soil_temperature_6_min_devstage_3',\n",
    " 'soil_temperature_6_vg_devstage_1',\n",
    " 'soil_temperature_6_vg_devstage_2',\n",
    " 'soil_temperature_6_vg_devstage_3',\n",
    " 'solar_radiation_devstage_1',\n",
    " 'solar_radiation_devstage_2',\n",
    " 'solar_radiation_devstage_3',\n",
    " 'wind_direction_devstage_1',\n",
    " 'wind_direction_devstage_2',\n",
    " 'wind_direction_devstage_3',\n",
    " 'wind_speed_avg_devstage_1',\n",
    " 'wind_speed_avg_devstage_2',\n",
    " 'wind_speed_avg_devstage_3',\n",
    " 'wind_speed_gusts_devstage_1',\n",
    " 'wind_speed_gusts_devstage_2',\n",
    " 'wind_speed_gusts_devstage_3',\n",
    " 'wind_speed_max_devstage_1',\n",
    " 'wind_speed_max_devstage_2',\n",
    " 'wind_speed_max_devstage_3'\n",
    "]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423efa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define predictors and target variable\n",
    "X = df.drop('sc_nir', axis=1)\n",
    "y = df['sc_nir']\n",
    "print(f\"We have {X.shape[0]} observations in our dataset and {X.shape[1]} features\")\n",
    "print(f\"Our target vector has also {y.shape[0]} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cacdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test set (train set: 70%, test set: 30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RSEED) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80d37650",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b44399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb89fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete pipeline for numerical features\n",
    "# apply transformers to numerical pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], sparse_threshold=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a38ac03",
   "metadata": {},
   "source": [
    "#### Transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape\n",
    "\n",
    "X_tf_train = tf.convert_to_tensor(X_train_transformed)\n",
    "y_tf_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "X_tf_test = tf.convert_to_tensor(X_test_transformed)\n",
    "y_tf_test = tf.convert_to_tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26098865",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training you need a train/val split (hopefully you did a train/test split before (and you should use the same as in your ML project to make results comparable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary to store results\n",
    "training_history = {}\n",
    "\n",
    "# define number of epochs and learning rate decay\n",
    "N_TRAIN = len(X_train)\n",
    "N_VAL = 0.2\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = N_TRAIN // 10\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "# lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "#     0.001,\n",
    "#     decay_steps=STEPS_PER_EPOCH*1000,\n",
    "#     decay_rate=1,\n",
    "#     staircase=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afda48b9",
   "metadata": {},
   "source": [
    "### Build, compile and fit your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path for new directory \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "# define function for creating a new folder for each run\n",
    "def get_run_logdir():\n",
    "    now = datetime.now()\n",
    "    run_id = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0728de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path where checkpoints should be stored\n",
    "checkpoint_path = \"training_1/ML_model.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=0) # Set verbose != 0 if you want output during training \n",
    "# return [list of your callbacks]\n",
    "def get_callbacks(name):\n",
    "    return tf.keras.callbacks.TensorBoard(run_logdir+name, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcdd8a",
   "metadata": {},
   "source": [
    "You can implement your callbacks in the `model.fit()` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile_and_fit(model, name, optimizer=None, max_epochs=EPOCHS):\n",
    "  \n",
    "    # model.compile\n",
    "    model.compile(optimizer = 'adam', loss = 'mae', metrics = ['mse'])\n",
    "    \n",
    "    # model.fit\n",
    "    history = model.fit(X_tf_train, y_train, batch_size = BATCH_SIZE, validation_split=N_VAL, epochs = max_epochs, callbacks=get_callbacks(name))\n",
    "    \n",
    "    # return results\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436f486",
   "metadata": {},
   "source": [
    "#### Build your model\n",
    "You can build your model by using `tf.keras.Sequential()` that helps you to sequentially define your different layers from input to output. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9e5d271",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31056e5",
   "metadata": {},
   "source": [
    "#### Train your model\n",
    "Train your model by using your `model_compile_and_fit()` function you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RSEED)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_tf_train,y_tf_train):\n",
    "\n",
    "#your_history = model_compile_and_fit(your_model, ....)\n",
    "    with tf.device('/cpu:0'):\n",
    "        training_history[\"first_model\"] = model_compile_and_fit(model, \"first_model\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0e282",
   "metadata": {},
   "source": [
    "#### Evaluate your model training\n",
    "TensorFlow offers now (this was more cumbersome before) a simple history plotter that you can use to plot training histories and see how the model performed on training and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5623bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function for MSE\n",
    "def plot_metric(history):\n",
    "    plt.plot(history.history['mse'])\n",
    "    plt.plot(history.history['val_mse'])\n",
    "    plt.ylim([0, 2.5])\n",
    "    plt.title('Model MSE')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c383836",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(training_history['first_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function for loss\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 5])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_history['first_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(X_tf_test, y_tf_test, verbose=0)\n",
    "print(f'Model MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3261a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_tf_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c71ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(y_test, y_pred):\n",
    "    \"\"\"Generated true vs. predicted values and residual scatter plot for models\n",
    "    Args:\n",
    "        y_test (array): true values for y_test\n",
    "        y_pred_test (array): predicted values of model for y_test\n",
    "    \"\"\"\n",
    "    # calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    # plot real vs. predicted values\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "    plt.subplots_adjust(right=1)\n",
    "    plt.suptitle('')\n",
    "    ax[0].scatter(y_pred, y_test, color=\"#2f7055\", alpha=0.7)\n",
    "    ax[0].plot([5, 35], [5, 35], color=\"#193251\")\n",
    "    ax[0].set_title(\"True vs. predicted values\", fontsize=16)\n",
    "    ax[0].set_xlabel(\"predicted values\", fontsize=14)\n",
    "    ax[0].set_ylabel(\"true values\",  fontsize=14)\n",
    "    ax[0].set_xlim((5), (y_pred.max()+10))\n",
    "    #ax[0].set_ylim((5), (y_test.max()+10))\n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax[1].scatter(y_pred, residuals, color=\"#2f7055\", alpha=0.7)\n",
    "    ax[1].plot([-400, 350], [0,0], color=\"#193251\")\n",
    "    ax[1].set_title(\"Residual Scatter Plot\", fontsize=16)\n",
    "    ax[1].set_xlabel(\"predicted values\", fontsize=14)\n",
    "    ax[1].set_ylabel(\"residuals\", fontsize=14)\n",
    "    ax[1].set_xlim((y_pred.min()-5), (y_pred.max()+5))\n",
    "    #ax[1].set_ylim((residuals.min()-5), (residuals.max()+5))\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de590e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_tf_test,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85938cce",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "926e2e1b",
   "metadata": {},
   "source": [
    "## Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3acc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# Second Model with more nodes\n",
    "#===========#\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])  \n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RSEED)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_tf_train,y_tf_train):\n",
    "#your_history = model_compile_and_fit(your_model, ....)\n",
    "    with tf.device('/cpu:0'):\n",
    "        training_history[\"second_model\"] = model_compile_and_fit(model, \"second_model\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(training_history['second_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be75b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_history['second_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa43a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(X_tf_test, y_tf_test, verbose=2)\n",
    "print(f'Model MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_tf_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_tf_test,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7469da8",
   "metadata": {},
   "source": [
    "## Third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c86ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# Third Model with more layers\n",
    "#===========#\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        # layers.Dense(500, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])  \n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RSEED)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_tf_train,y_tf_train):\n",
    "\n",
    "#your_history = model_compile_and_fit(your_model, ....)\n",
    "    with tf.device('/cpu:0'):\n",
    "        training_history[\"third_model\"] = model_compile_and_fit(model, \"third_model\")\n",
    "\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc45f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(training_history['third_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_history['third_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061877f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(X_tf_test, y_tf_test, verbose=2)\n",
    "print(f'Model MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5340430",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_tf_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_tf_test,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6e4590f",
   "metadata": {},
   "source": [
    "## Fourth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# Fourth Model with dropout\n",
    "#===========#\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        layers.Dense(1)\n",
    "    ])  \n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=RSEED)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_tf_train,y_tf_train):\n",
    "\n",
    "#your_history = model_compile_and_fit(your_model, ....)\n",
    "    with tf.device('/cpu:0'):\n",
    "        training_history[\"fourth_model\"] = model_compile_and_fit(model, \"fourth_model\")\n",
    "\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(training_history['fourth_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_history['fourth_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d676da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(X_tf_test, y_tf_test, verbose=2)\n",
    "print(f'Model MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_tf_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_tf_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6251b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./my_logs --port=6005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "61433a13eba5a829a1dfb9a19c2018b676dc6600a268347de5770ba407739072"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
